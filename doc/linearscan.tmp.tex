%% \documentclass{article}
\documentclass{llncs}

\usepackage{makeidx}  % allows for indexgeneration
\usepackage[usenames,dvipsnames]{color}
% \usepackage{fancyhdr}  \pagestyle{fancy}
% llncs wants us to user verbatim package (yuck): \usepackage{fancyvrb} 
\usepackage{verbatim}
\usepackage{environ}

\newif\ifdraft\draftfalse  % draft = comments and half-baked bits
%\newif\ifdraft\drafttrue  % draft = comments and half-baked bits

\newenvironment{MyCoqExample}{\small \verbatim}{\endverbatim \normalsize}
\newenvironment{MyCoqExampleStar}{\small \verbatim}{\endverbatim \normalsize}
\newenvironment{MyCoqUneval}{\small \verbatim}{\endverbatim \normalsize}
\NewEnviron{MyCoqEval}{}

% Comments
\newcommand{\xcomment}[3]{\ifdraft\textcolor{#1}{[#2: #3]}\else\fi}
\newcommand{\fixme}[1]{\xcomment{red}{FIXME}{#1}}
\newcommand{\todo}[1]{\xcomment{red}{TO DO}{#1}}
\newcommand{\gts}[1]{\xcomment{OliveGreen}{GTS}{#1}}

% LLNCS style does not allow for generating PDF bookmarks.
% should be the last one
\usepackage[%
    pdftex,%
    %pdfpagelabels,%
    bookmarks=true,
    bookmarksopen=true,
    % pdfdisplaydoctitle=true,%
    % pdflang=en-us,%
    % pdfencoding=auto,%
    % bookmarksnumbered=true,%
]{hyperref}

% Instructions from Laurie Walsh, 11/12/2014: Footer should contain:
% Title page: "Approved for Public Release; Distribution Unlimited. Cleared for Open Publication on 11/12/14."
% Subsequent pages, in the footer: "Non-Technical Data - Releasable to Foreign Persons."
% \fancyhf{}    % clear out all head and foot
% \lfoot{Approved for Public Release; Distribution Unlimited. Cleared for Open Publication on 11/12/14.}
% \renewcommand{\headrulewidth}{0.0pt}

\begin{document}

% LLNCS:
\frontmatter          % for the preliminaries

\pagestyle{headings}  % switches on printing of running heads

% LLNCS:
\mainmatter

\title{Formalizing a Register Allocator in Coq}

\author{John Wiegley \and Gregory Sullivan}

% LLNCS
\institute{BAE Systems, Burlington MA 01803, USA}

\maketitle
% \thispagestyle{fancy}

\begin{abstract}
  This experience report describes the process of formalizing a register
  allocation algorithm using the Coq proof assistant, and extraction to a
  Haskell implementation.  We focus on how the Coq specification evolved from
  initial attempts---which seemed straightforward but later became
  unwieldy---to a final version that allowed for much easier proof.  Our goal
  is to assist other teams who may be considering similar verification
  efforts.
  
% LLNCS:
\keywords{formal verification, register allocation, Coq, Haskell}
\end{abstract}

\section{Introduction}
\label{sec:intro}

As part of implementing a Haskell-based cross compiler, from a C-like language
named \emph{Tempest} to a new hardware architecture called \emph{SAFE}, we
settled on the linear scan register allocation algorithm described by Wimmer
and M\"{o}ssenb\"{o}ck~\cite{Wimmer:2005}.

To gain confidence in our implementation (consistent with other formal
verification efforts on the larger SAFE project\footnote{See
  \url{http://www.crash-safe.org/} for more information on the SAFE project.})
we have formalized the algorithm in Coq~\cite{coq_manual}. Because the rest of
our cross-compilation toolchain is implemented in Haskell, we extract Haskell
code from the Coq development.

Our goal is to take the paper by Wimmer and M\"{o}ssenb\"{o}ck as a
specification (in English and pseudo-code) and formalize the algorithm in Coq
so that \emph{only correct implementations type-check}.  After that, the only
parameters of choice should be choosing which efficiency dimensions to
optimize for.

The Coq definitions and proofs, and the generated Haskell code can be found at
\url{https://github.com/jwiegley/linearscan}.

% \clearpage
% \lfoot{Non-Technical Data - Releasable to Foreign Persons.}

\begin{coq_eval}
Require Import ScanState.
Require Import SSMorph.
Require Import Machine.
Require Import Lib.

Set Implicit Arguments.
Unset Strict Implicit.
Unset Printing Implicit Defensive.
Generalizable All Variables.

Module MyMachine <: Machine.

Definition maxReg := 32.

Lemma registers_exist : (maxReg > 0).
Proof. unfold maxReg. exact: ltn0Sn. Qed.

End MyMachine.

Module Import M := MScanState MyMachine.
Module Import MS := MSSMorph MyMachine.

Definition step1 `(st : ScanState sd)
  : { sd' : ScanStateDesc | ScanState sd' } :=
  exist _ sd st.

Definition step2 := @step1.
Definition step3 := @step1.
Definition step4 := @step1.
Definition final := @step1.

Definition moveActiveToHandled `(x : IntervalId sd) : ScanStateDesc.
Admitted.

Definition moveActiveToInactive `(x : IntervalId sd) : ScanStateDesc.
Admitted.

Definition transportId `(H : nextInterval st = nextInterval st')
  (x : IntervalId st) : IntervalId st'.
Admitted.
\end{coq_eval}

\section{Design evolution}
\label{sec:evolve}

\subsection{First design: Purely computational}
\label{sec:compdesign}

In the first iteration, we approached the task much as any functional
programmer might: data types carry computationally relevant information,
functions are implemented in a straightforward fashion, and proofs are in
terms of those types and functions.  There are almost no dependent types
involved, and very few inductive types.  Most of the types used are records.

We quickly encountered two issues with this ``purely computational'' approach:
\begin{enumerate}
\item Proving termination of complex recursive functions is difficult.
\item Proving data structure invariants requires proving that functional
  updates maintain invariants. If we modify that structure, we must revisit
  many proofs.
\end{enumerate}
The following function definition illustrates the above
issues. \texttt{checkActiveIntervals} loops over the current set of
intervals (an interval corresponds to the range of program locations
where a single ``virtual register'' is live) and moves selected
intervals into either the ``handled'' or ``inactive'' sets.

\begin{coq_example*}
Definition checkActiveIntervals st pos : ScanStateDesc :=
  let fix go st st0 (ints : list (IntervalId st)) (pos : nat) :=
    match ints with
    | nil => st0
    | x :: xs =>
        let i := getInterval x in
        let st1 := if intervalEnd i < pos
                   then moveActiveToHandled x
                   else if pos < intervalStart i
                        then moveActiveToInactive x
                        else st0 in
        go st st1 xs pos
    end in
  go st st [seq fst i | i <- active st] pos.
\end{coq_example*}

Now consider the following two Lemmas, which claim that (1)
\texttt{checkActiveIntervals} does not disrupt the next interval to handle;
and (2) \texttt{checkActiveIntervals} does not introduce any new intervals.

\begin{coq_example*}
Lemma checkActiveIntervals_spec1 : forall st st' pos,
  st' = checkActiveIntervals st pos
    -> nextInterval st = nextInterval st'.
\end{coq_example*}
\begin{coq_eval}
Admitted.

Require Import Coq.Lists.List.
\end{coq_eval}

\begin{coq_example*}
Lemma checkActiveIntervals_spec2 : forall st st' i pos
  (H : st' = checkActiveIntervals st pos),
  ~ In i (all_state_lists st)
    -> ~ In (transportId (checkActiveIntervals_spec1 H) i)
            (all_state_lists st').
\end{coq_example*}
\begin{coq_eval}
Admitted.
\end{coq_eval}

Because \texttt{checkActiveIntervals} operates by generating a new scan state
on \emph{each} iteration of the loop, we need to prove that all properties are
maintained by each helper function.

As a result, we found ourselves having to break up functions further, and
prove more auxiliary lemmas about each piece.  After losing much time to this
approach, we decided that evidence should be carried through each modification
to prove the desired property.  That is, rather than proving correctness
``from the outside'', the function was changed to produce the necessary
evidence (as proof terms) needed by subsequent lemmas.

\begin{coq_eval}
Require Import Coq.Arith.Compare_dec.
Require Import Coq.Lists.List.
Require Import Coq.Program.Wf.
Require Import Coq.Sorting.Permutation.
Require Import Coq.Sorting.Sorted.

Definition gtb x y :=match nat_compare x y with Gt => true | _ => false end.

Definition definition_goes_here {a : Type} : a. Admitted.
\end{coq_eval}

\subsubsection*{Lesson learned.}

Proving purely computational algorithms of this complexity meant using proof
scripts to explore every possibility at each step of the algorithm: in effect,
re-implementing the same algorithm in reverse, only now using tactics.

\subsection{Second design: Proof-carrying data}
\label{sec:depinduct}

In the second design of our algorithm, an attempt was made to reduce the
complexity of proof scripts by adding evidence to the primary data structures,
so that such evidence would be available wherever needed.  Take for example an
excerpt from an earlier version of the \texttt{ScanState} data structure,
which records the overall state of several intermediary lists:

\begin{coq_example*}
Record ScanStateV2 := {
    unhandled : list (nat * nat);
    active    : list nat;
    inactive  : list nat;
    handled   : list nat;

    unhandled_sorted : StronglySorted (lebf (@snd _ _)) unhandled;
    lists_are_unique : uniq (map (@fst _ _) unhandled ++ active ++
                             inactive ++ handled)
}.
\end{coq_example*}

In this structure we have four working lists, and two properties about
the lists. Property \texttt{unhandled\_sorted} requires that the
\texttt{unhandled} list is sorted by start position
(2\textsuperscript{nd} element of each pair in list), and property
\texttt{lists\_are\_unique} states that no interval index recurs
across the four lists.

\subsubsection*{Lessons learned.}

While the idea of simultaneously maintaining data structures and
invariants of those data structures is good, mixing data and
propositions into one type does not quite work out, for the following
reasons: 

\begin{itemize}
\item It conflates data with predicates, which is generally a poor choice,
  because the number of predicates often grows over time.

\item It forces the propositions to be considered whenever a value is mutated
  or created, even though it may be preferable to transform such values
  successively through different functions, and prove correctness of the
  composed mutation afterward.

  For example, if one were to pass around a sorted list structure, which
  combined a list with a proof of sortedness, it would disallow the
  possibility of unsorted intermediate state, which may still be corrected if
  the overall operation can prove that sortedness is maintained.
\end{itemize}

A guiding principle is to separate concerns as much as practical, which led
directly to the third redesign, discussed in the next section.  

\subsection{Third design: Proof-Qualified Data (Regular Datatypes Qualified by
  Inductive Propositions)}
\label{sec:splitdesign}

After several iterations, where proving was found to be difficult and the
resulting code looked quite unlike its non-dependent alternatives, we
developed a methodology that is the primary message of this experience report.
Note that this methodology is in no way novel or unknown, but we find it
under-documented in the tutorial literature on Coq and so present it here to
benefit others seeking to use Coq for verified programming tasks:

\begin{enumerate}
\item Design data-carrying types as inductive data types and records in
  \texttt{Set}, much like in a non-dependently typed language.

\item Constrain how such types may be constructed and mutated by encoding them
  as inductive propositions (in \texttt{Prop}).  For example, the
  \texttt{StronglySorted} inductive predicate found in the Coq standard
  library.

\item High-level functions should take and produce data qualified by these
  propositions; lower-level worker functions may work on unqualified,
  intermediate values, so long as the composition is qualified.

\item Prove theorems about the properties required of the data, by induction
  on values qualified by the predicate(s).

\end{enumerate}

Thus, the third iteration of our design divided each of the core data types
into pairs:

\begin{itemize}
\item A non-dependent record, carrying computationally-relevant content only,
  in \texttt{Set}.

\item Dependent, inductive types, carrying propositionally-relevant content
  only, in \texttt{Prop}.
\end{itemize}

The pairing of the two provides the ``proof-carrying data'' that was
originally desired in the second design, but in a manner allowing for simple
transformation of the underlying data when necessary, and induction on
propositional evidence as required.  The division of labor clarified not only
the proofs, but made many constructions far simpler to manage.  This is the
final design that was settled upon, with one additional twist, to be continued
in the next section.

\subsection{Examples}
\label{sec:examplesv3}

As before, we define a data type \emph{ScanStateDesc} that contains
only data (no propositions). An excerpt is:
\begin{MyCoqUneval}
Record ScanStateDesc : Type := {
    unhandled : list (IntervalId * nat);   (* starts after pos *)
    active    : list IntervalId;           (* ranges over pos *)
    inactive  : list IntervalId;           (* falls in lifetime hole *)

    all_state_lists := map fst unhandledIds ++
                       active ++ inactive ++ handled
}.
\end{MyCoqUneval}

Next, we define an inductive proposition named \texttt{ScanState}(of type
Proposition, indexed by \texttt{ScanStateDesc}) that enumerates exactly the
allowed methods for transitioning from one \emph{ScanStateDesc} record
satisfying \texttt{ScanState} to another \emph{ScanStateDesc} record
satisfying \texttt{ScanState}. Following are some excerpts:

\begin{MyCoqUneval}
Inductive ScanState : ScanStateDesc -> Prop :=
  | ScanState_nil :
    ScanState
      {| ... to fields to 0 or nil... |}
...
  | ScanState_moveActiveToHandled sd :
    ScanState sd -> forall `(H : x \in active sd),
    ScanState
      {| active           := rem x (active sd)
       ; handled          := x :: handled sd
         ... other fields unchanged ... |}
\end{MyCoqUneval}
     
Now, if we want to prove a theorem about a \texttt{ScanState}-qualified
\texttt{ScanStateDesc} record, we consider each case in the definition of
\texttt{ScanState}.

\section{Chains of Evidence Using Monads}
\label{sec:pfmorph}

During the course of development, it became clear that not only was evidence
required of certain functions, but also a proof that this evidence maintains a
special relationship with some initial condition.  The primary case of this
was proving well-foundedness of the main algorithm.

Well-foundedness may be demonstrated in Coq 8.4 by establishing a mapping from
some input argument to a natural number, and then proving that this value only
decreases with each recursive call.  If we further view an algorithm as a
composition of smaller functions, then each member of the composition must
either preserve this value, or cause it to decrease at least once.

In order to prove that order is maintained, and that we are ultimately
``productive'' (i.e., a decrease in the initial value, the requirement of
well-foundedness), we encode this requirement in the type of each composed
function.  That is, the type must show that the function's input is $\le$ the
initial value, and that the output is $\le$ or $<$ the initial value.  If at
least one output is $<$, and each subsequently composed function is only
$\le$, we have demonstrated well-foundedness overall.

Such specialized composition---where a context is preserved across function
calls---is a notion captured exactly by monads from category theory.  What we
are describing above is specifically an \emph{indexed state monad}, where a
certain state (a relation with our initial value) is maintained throughout the
composition, and we may specify for each function what its input and output
requirements are of this relation.

Here is the sort of function type we end up with:

\begin{coq_example*}
Definition handleInterval {pre : ScanStateDesc}
  : SState pre SSMorphHasLen SSMorphSt (option PhysReg).
\end{coq_example*}

% Notes from talking with John
% ----------------------------
% SState is indexed monad for proving things about a state w.r.t. pre
% (synonym for indexed monad).  Pre is a ScanStateDesc.

% SState relates pre to ingoing and outgoing states.

% SSMorphHasLen (a predicate from ScanStateDesc to Prop) incoming
% state is related to pre in that has work to do
% (specif. SState.len(unhandled) > 0).  if pre had work to do, then
% SState still has work to do.

% SSMorphSt (a predicate from ScanStateDesc to Prop) (should be
% SSMorphProductive) says have reduced work to be done.

% (option PhysReg) says result of the function may be an allocation of
% a physical register (or we failed to allocate a physical register).

This type says: this function accepts some incoming state which is related to
an original state \texttt{pre} in that it still has work to done
(\texttt{SSMorphHasLen}); its output state reduces the scope of this work
(\texttt{SSMorphSt}).  The result value is a physical register, if one was
able to be allocated.  Note that this signature tells us that calling this
function provides sufficient evidence to prove termination (due to the
\texttt{SSMorphSt} relation), as will any composition including this function.

Working with a monad like this permits the management of evidentiary state to
happen ``behind the scenes'', clarifying the code by removing explicit proof
management.  In the version of our implementation before this change, proving
these relationships became quite time-consuming, and obscured the real work
being done.

By this means, the final well-foundedness proof of the main algorithm became a
one-liner, while at the same time each component function of the algorithm was
greatly simplified, reading similarly to an equivalent Haskell implementation.

\section{Code extraction}
\label{sec:extract}

A primary goal of this endeavor, from the very beginning, was extraction to
Haskell.  The question was whether Coq and formal methods would be up to the
task of building a Haskell library in a better way.

About midway through the development, focus was turned toward producing
reasonable Haskell code through Coq's built-in extraction facilities.  These
allow for:

\begin{itemize}
\item Extraction of functions and types (those not in Prop), and their related
  definitions.

\item Association of some types and constructors with native Haskell types.

\item Renaming some functions to use their direct Haskell counterparts,
  requiring that one implicitly trust these to be both total and correct.
\end{itemize}

There is unfortunately no published meta-theory for the extraction
process from Coq to Haskell, and indeed some bugs were encountered
during the course of this project (including one where type variables
were inexplicably swapped in a constructor whenever auto-inlining was
used).

Even still, the ability to write code in Coq and have it so easily converted
to naive Haskell was incredibly useful, and an area that we hope sees further
development.  The resulting Haskell code was quite simple and straightforward,
so that it is amenable to optimization by the GHC compiler.

% ---- Bibliography ----
\bibliographystyle{splncs03}
\bibliography{../../safe}

\end{document}

%  LocalWords:  ssenb Wimmer ScanState SSMorph Generalizable MyMachine maxReg
%  LocalWords:  ltn Qed MScanState MSSMorph sd ScanStateDesc IntervalId pos
%  LocalWords:  moveActiveToHandled moveActiveToInactive transportId ints xs
%  LocalWords:  nextInterval checkActiveIntervals getInterval intervalEnd fst
%  LocalWords:  intervalStart forall gtb ScanStateV OldScanState unhandled nd
%  LocalWords:  StronglySorted lebf uniq sortedness Datatypes unhandledIds
%  LocalWords:  handleInterval SState SSMorphHasLen SSMorphSt PhysReg
