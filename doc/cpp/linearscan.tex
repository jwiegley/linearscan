% jww (2015-08-05): Remove preprint option once paper is in final form.
\documentclass[preprint]{sigplanconf}

\usepackage{amsmath}            % equations and math symbols
\usepackage{enumitem}           % \setitemize, [noitemsep]

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

%\setitemize{noitemsep,topsep=2pt,partopsep=2pt}

\conferenceinfo{CPP 2016}{January 20--22, 2016, Saint Petersburg, Florida, USA} 
\copyrightyear{2016} 
%\copyrightdata{978-1-nnnn-nnnn-n/yy/mm} 
%\doi{nnnnnnn.nnnnnnn}

% Uncomment one of the following two, if you are not going for the 
% traditional copyright transfer agreement.
\exclusivelicense                % ACM gets exclusive license to publish, 
                                 % you retain copyright
%\permissiontopublish             % ACM gets nonexclusive license to publish
                                  % (paid open-access papers, 
                                  % short abstracts)

% These are ignored unless 'preprint' option specified.
\titlebanner{Initial draft v1.0}
\preprintfooter{Experience report building a register allocator in Coq}

\title{Formalizing a Register Allocator in Coq}
\subtitle{Experience Report}

\authorinfo{John Wiegley}
           {BAE Systems}
           {john.wiegley@baesystems.com}
\authorinfo{Howard Reubenstein}
           {BAE Systems}
           {howard.reubenstein@baesystems.com}

\maketitle

\begin{abstract}
  This experience report describes the development of a register allocation
  algorithm in Coq and its subsequent inclusion in a larger Haskell project.
  It documents the use of Coq from an engineering point of view (i.e., not
  solely for proof work), and by a team initially unfamiliar with using Coq
  for programming tasks. It presents multiple approaches to software design
  within a dependently typed language, the hurdles encountered and how they
  were overcome, and pitfalls others should consider if contemplating Coq for
  such a task. Its conclusion is that Coq is an excellent functional language,
  for which up-front knowledge of certain techniques is essential to reducing
  costs. It is hoped the reader may save some of the time spent discovering
  these techniques, for though most of them are documented, it can be
  difficult to know what is important before experiencing the need.
\end{abstract}

\category{D.2.4}{Software Verification}
                {Formal methods and validation}
                [Programming in Coq]

% % general terms are not compulsory anymore, you may leave them out
% \terms
% term1, term2

\keywords
formal methods, verification, register allocator, Coq, Haskell

\section{Introduction}
\label{sec:intro}

Coq is a dependently-typed language and theorem proving environment well-known
for its ability to prove properties of algorithms. This suggested it would be
ideal for a register allocator implementation, which is easily expressed as a
function from inputs to outputs. Since the authors had not used Coq for
engineering previously, what was unknown is that Coq requires certain ways of
approaching different problems, or else the time obligations of proof can
become prohibitive. This report presents some of these pitfalls, so that
others may avoid them.

Further, since the desired goal was to integrate the resulting functions
within a compiler written in Haskell, Coq was also chosen for its ability to
extract code directly to Haskell, despite reports that the extraction
mechanism may be weak in this area. It was decided to rely on unit tests to
verify expectations of the extracted code.

\section{First approach: Complex types}

In order to prove properties about functions operating on types, the first
approach was to encode certain properties in the types themselves, to prevent
invalid values from being constructed or consumed. This does not guarantee
every behavior of the functions involved, but does restrict what values may
flow through the program.

For example, a central type in the allocator is \verb|ScanState|, which
governs the state of four related lists that evolve as the algorithm iterates
through the submitted program graph:

\begin{description}[noitemsep]
\item[Unhandled intervals] Code spans where a variable use exists but no
  register has been allocated;
\item[Active intervals] Where an allocation has been made and that cover the
  current position;
\item[Inactive intervals] Which have an allocation but do \emph{not} cover the
  current position---thus they may be easily split and their register spilled
  to relieve pressure;
\item[Handled intervals] Which have been finally allocated: no further
  changes are made.
\end{description}

Several relationships exist among these lists (this list is not exhaustive):

\begin{itemize}
\item Every interval must appear in exactly one list;
\item An interval may be split and/or moved to another list, but cannot be
  removed;
\item New intervals may be added to any list, but for the unhandled list must
  be inserted after the current position (this is how termination is proved);
\item A register cannot be allocated to more than one active interval;
\item No two intervals allocated to the same register may overlap.
\end{itemize}

By maintaining these properties, it is guaranteed that registers are available
when allocations are made. However, it still does not ensure that every
variable use is covered by an interval, as intervals alone are used to decide
allocations. Such a requirement occurs prior to allocation, in the phase that
builds up the unhandled interval list.

In this brief example, a simple collection of four lists involves a fairly
complex web of relationships. The initial attempt at verification was to
enrich \verb|ScanState| with theorems stating each requirement, so no invalid
states could exist. This drives the code manipulating \verb|ScanState|'s to
demonstrate correctness of every modification made to the allocation state.

This approach fails exactly where it succeeds: \emph{every modification
  requires proof of validity}. Sometimes, it is convenient to allow
intermediate states within a function to be invalid, so long as the result is
valid. By embedding theorems within the type, it is never possible to extract
an interval from a list, manipulate it separately, and then re-insert it. It
can only be moved in one atomic step, to preserve the invariant that the total
interval count is never reduced.

It did not take long before complex algorithms became unwieldy to write in
this style. Proof requirements seemed to multiply with each new function,
resulting in byzantine chains of evidence to justify each new transformation.

\section{Second approach: Simple types}

In order to tackle the Gordian knot created by involving theorems with types,
the next attempt dealt with simple, non-dependent types only, with external
theorems to relate application functions to those types.

<This was bad because it caused combinatorial expansion, necessary evidence
was missing, etc.>

\section{Third approach: Hybrid types}

% * Setting the stage
% 
% ** Needed a new register allocator due to subtle bugs in the previous one
% ** Chose Coq to provide verification and fidelity of results
% ** No prior experience using Coq to build production software
% ** Deployment was chosen to be extraction to Haskell as a library
% 
% * Initial effort: Complex types
% 
% ** Primary types were records of both data and propositions
% ** The resulting dependent types imposed many proof obligations
% 
% * Next effort: External proofs
% 
% ** Propositions were moved from types into theorems
% ** Proving required complex induction over large functions
% ** Every change required proofs to be updated
% 
% * Next effort: Data types + Propositional types
% 
% ** Propositions were moved to constructors of an inductive data type
% ** This divided between data and constructive predicates on the data
% ** Several fruits of this effort were kept, but imposed work
% 
% * Next effort: Proof by reflection
% 
% ** Many propositions were exchanged for functions returning bool
% ** Switched to the ssreflect library, which supports this type of reasoning
% ** Allowed many proofs to avoid induction and use mainly rewriting
% 
% * Finalization of library code
% 
% ** Abstracted library code lent itself well to reflection and proof
% ** Subsystems once "completed" needed little attention thereafter
% ** This where Coq shined, eliminating tests and creating confidence
% 
% * Continued work on application code
% 
% ** Higher level code on top of libraries needed frequent change
% ** As the design evolved, many functions needed to be rewritten
% ** Owing to inductive predicates, proof obligations required constant work
% ** As the code evolved, changes became more and more difficult to make
% ** The last 20% of the work took an inordinate amount of time as a result
% 
% * Pressure release: Error results to avoid proof obligation
% 
% ** To recover the pace of development, functions were permitted to "error out"
% ** Reporting error, instead of proving correctness, brought back flexibility
% ** When errors became common, it justified the proof work to eliminate them
% 
% * Pressure release: Coq as a functional language
% 
% ** Most new functions abandoned proof restrictions and dependent types
% ** In this sense, Coq was just being used as if it were a "stricter Haskell"
% ** Core libraries carried high assurance, but the new code now requried testing
% 
% * The Haskell bridge
% 
% ** Not all constructions could be made efficient on both sides
% ** Proof requirements do not prefer complex implementations!
% ** Monads were introduced to allow effect-producing callbacks
% ** Much work was done to optimize extraction, but led to difficult bugs
% 
% * Overall experience
% 
% ** Coq would have been ideal for core library development
% ** Imposed a high engineering cost for application development
% ** Since not everything was proved, testing was still necessary
% ** Mounting time restrictions led to fewer and fewer proofs
% ** The final product required testing as with any application
% ** However, confidence levels at seeing all test pass was high
% 
% * Take-aways
% 
% ** Coq is a powerful functional platform for crafting code of value
% ** It is not the right tool for large, complex applications
% ** Not unless one abandons most of what makes it so powerful
% ** A combination of Haskell and Coq would be of great value
% ** Getting this combination to work well requires some investment

\appendix
\section{Appendix Title}

This is the text of the appendix, if you need one.

\acks

Acknowledgments, \cite{TotallyFree} if needed.

\bibliographystyle{abbrvnat}
\bibliography{linearscan}

\end{document}

%  LocalWords:  ssenb Wimmer ScanState SSMorph Generalizable MyMachine maxReg
%  LocalWords:  ltn Qed MScanState MSSMorph sd ScanStateDesc IntervalId pos
%  LocalWords:  moveActiveToHandled moveActiveToInactive transportId ints xs
%  LocalWords:  nextInterval checkActiveIntervals getInterval intervalEnd fst
%  LocalWords:  intervalStart forall gtb ScanStateV OldScanState unhandled nd
%  LocalWords:  StronglySorted lebf uniq sortedness Datatypes unhandledIds
%  LocalWords:  handleInterval SState SSMorphHasLen SSMorphSt PhysReg
