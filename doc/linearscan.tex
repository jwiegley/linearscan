\documentclass{llncs}

\usepackage{makeidx}  % allows for indexgeneration
\usepackage[usenames,dvipsnames]{color}
\usepackage{fancyvrb} 
\usepackage{environ}

% \newif\ifdraft\draftfalse  % draft = comments and half-baked bits
\newif\ifdraft\drafttrue  % draft = comments and half-baked bits

\DefineVerbatimEnvironment{MyCoqExample}{Verbatim}{frame=lines,framerule=1pt}
\DefineVerbatimEnvironment{MyCoqExampleStar}{Verbatim}{frame=lines,framerule=1pt}
% \DefineVerbatimEnvironment{MyCoqEval}{Verbatim}{}
\NewEnviron{MyCoqEval}{}

% Comments
\newcommand{\xcomment}[3]{\ifdraft\textcolor{#1}{[#2: #3]}\else\fi}
\newcommand{\fixme}[1]{\xcomment{red}{FIXME}{#1}}
\newcommand{\todo}[1]{\xcomment{red}{TO DO}{#1}}
\newcommand{\gts}[1]{\xcomment{OliveGreen}{GTS}{#1}}

% should be the last one
\usepackage[%
    %pdftex,%
    %pdfpagelabels,%
    %bookmarks=true,%
    % pdfdisplaydoctitle=true,%
    % pdflang=en-us,%
    % pdfencoding=auto,%
    % bookmarksnumbered=true,%
]{hyperref}

\begin{document}

\frontmatter          % for the preliminaries

\pagestyle{headings}  % switches on printing of running heads

\mainmatter

\title{Experience Report: Implementing a register allocator for Haskell
  in Coq}

\author{John Wiegley \and Gregory Sullivan}

\institute{BAE Systems, Burlington MA 01803, USA}

\maketitle

\begin{abstract}
  We describe the process of formalizing a register allocation algorithm, and
  its correctness properties, in the Coq proof assistent, and subsequent
  extraction of a Haskell implementation.  Rather than simply state the final
  specification in Coq, as if we had constructed it in one pass, we focus on
  the process of evolving the Coq specification from our initial
  attempts---which seemed straightforward but later became unwieldy---to a
  final version that allowed for much easier proof.  Our goal in this
  experience report is to assist other teams who may be considering similar
  algorithm verification efforts, in the hope that those teams can learn from
  our mistakes.
  
 \keywords{formal verification, register allocation, Coq, Haskell}
\end{abstract}

\section{Introduction}
\label{sec:intro}

As part of implementing a cross compiler from a C-like language named
\emph{Tempest} to a new hardware architecture called \emph{SAFE}, we settled
on the linear scan register allocation algorithm described by Wimmer and
M\"{o}ssenb\"{o}ck~\cite{Wimmer:2005}.

To gain confidence in our implementation, consistent with other formal
verification efforts on the larger SAFE project\footnote{See
  \url{http://www.crash-safe.org/} for more information on the SAFE project.},
we have formalized the algorithm in Coq~\cite{coq_manual}. Because the rest of
our cross-compilation toolchain is implemented in Haskell, we extract Haskell
code from the Coq development.

Our goal is to take the paper by Wimmer and M\"{o}ssenb\"{o}ck as a
specification (in English and pseudo-code) and formalize the algorithm in Coq
so that \emph{only correct implementations typecheck}.  After that, the
only parameters of choice are choosing which efficiency dimensions to
optimize for.

\fixme{Rewrite section overview}In Section~\ref{sec:alg}, we give a
brief overview of the linear scan register allocation algorithm and
its datatypes, focusing on correctness properties. In
Section~\ref{sec:evolve}, we describe the design iterations as we
discovered strategies for applying Coq to this sort of combined
verification and implementation task.  We conclude with lessons
learned, in the hope that our experience will help others embrace the
methodology of formal development, proof, and code extraction (while
avoiding some of the pitfalls).

\todo{Add pointer to github repo containing all proofs and extracted
  Haskell (and maybe extended version of this paper).}

\begin{MyCoqEval}
Require Import ScanState.
Require Import SSMorph.
Require Import Machine.
Require Import Lib.
Require Import Omega.
Require Import Coq.Lists.List.

Set Implicit Arguments.
Unset Strict Implicit.
Unset Printing Implicit Defensive.
Generalizable All Variables.

Module MyMachine <: Machine.

Definition maxReg := 32.

Lemma registers_exist : (maxReg > 0).
Proof. unfold maxReg. ssomega. Qed.

End MyMachine.

Module Import M := MScanState MyMachine.
Module Import MS := MSSMorph MyMachine.

Definition step1 `(st : ScanState sd)
  : { sd' : ScanStateDesc | ScanState sd' } :=
  exist _ sd st.

Definition step2 := @step1.
Definition step3 := @step1.
Definition step4 := @step1.
Definition final := @step1.

Definition moveActiveToHandled `(x : IntervalId sd) : ScanStateDesc.
Admitted.

Definition moveActiveToInactive `(x : IntervalId sd) : ScanStateDesc.
Admitted.

Definition transportId `(H : nextInterval st = nextInterval st')
  (x : IntervalId st) : IntervalId st'.
Admitted.
\end{MyCoqEval}

\section{Design evolution}
\label{sec:evolve}

\subsection{First design: Purely computational}
\label{sec:compdesign}

In the first iteration, we approached the task much as any functional
programmer might: data types carry computationally relevant
information, functions are implemented in a straightforward fashion,
and proofs are in terms of those types and functions.  There are
almost no dependent types involved, and very few inductive types.
Most of the types used are records.

% In the beginning this was a relatively simple and painless approach,
% until the requirement of proving termination of the top-level routine
% of the linear scan algorithm. In Coq, all functions have to be proven
% to terminate. 
% the primary Fixpoint
% definition: the entry-point to the linear scan algorithm.

We quickly encountered two issues with this ``purely computational''
approach:
\begin{enumerate}
\item Proving termination of complex recursive functions is difficult.
\item Proving data structure invariants requires proving that mutating
  function maintains invariants. If we modify that structure, we must revisit
  many proofs.
\end{enumerate}
The following function definition illustrates the above
issues. \texttt{checkActiveIntervals} loops over the current set of
intervals (an interval corresponds to the range of program locations
where a single ``virtual register'' is live) and moves selected
intervals into either the ``handled'' or ``inactive'' sets.

\begin{MyCoqExampleStar}
Definition checkActiveIntervals st pos : ScanStateDesc :=
  let fix go st st0 (ints : list (IntervalId st))
             (pos : nat) :=
    match ints with
    | nil => st0
    | x :: xs =>
        let i := getInterval x in
        let st1 := if intervalEnd i <? pos
                   then moveActiveToHandled x
                   else if pos <? intervalStart i
                        then moveActiveToInactive x
                        else st0 in
        go st st1 xs pos
    end in
  go st st (active st) pos.
\end{MyCoqExampleStar}

% At first this seemed straightforward to prove, but quickly became intractable.
% In effect, proving a purely computational algorithm of this complexity meant
% using tactics to explore every possibility at each step of the algorithm: in
% effect, re-implementing the same algorithm in reverse, only now in the tactics
% language.

% This led to the realization that greater structure at the type level would be
% necessary to capture evidence determined via computation at the lower levels
% of the algorithm, so that it could percolate back up to the higher levels
% where it was needed.

% Case in point driving this need was a function used to move active intervals
% to either the inactive or unhandled lists:

Now consider the following two Lemmas, which claim that (1)
\texttt{checkActiveIntervals} does not disrupt the next interval to handle;
and (2) \texttt{checkActiveIntervals} does not introduce any new intervals.

% This required some supporting lemmas, to show that certain properties of the
% scan state were preserved by the function:

\begin{MyCoqExampleStar}
Lemma checkActiveIntervals_spec1 : forall st st' pos,
  st' = checkActiveIntervals st pos
    -> nextInterval st = nextInterval st'.
\end{MyCoqExampleStar}
\begin{MyCoqEval}
Admitted.
\end{MyCoqEval}

\begin{MyCoqExampleStar}
Lemma checkActiveIntervals_spec2 : forall st st' i pos
  (H : st' = checkActiveIntervals st pos),
  ~ In i (all_state_lists st)
    -> ~ In (transportId (checkActiveIntervals_spec1 H) i)
            (all_state_lists st').
\end{MyCoqExampleStar}
\begin{MyCoqEval}
Admitted.
\end{MyCoqEval}

Because \texttt{checkActiveIntervals} operates by generating a new scan state
on \emph{each} iteration of the loop, we need to prove that all properties are
maintained by each helper function.

% However, both of these became inordinately difficult to prove, since we are
% folding over a list of items taken from the original scan state, and using
% each item to generate new, intermediate scan states.  This is further done by
% a fixpoint within the function, making it difficult for Coq to simplify.

As a result, we found ourselves having to break up functions further, and
prove more auxiliary lemmas about each piece.  After losing much time to this
approach, we decided that evidence should be carried through each modification
to prove the desired property.  That is, rather than proving correctness
``from the outside'', the function was changed to manage evidence, which
removed the need for helper lemmas.

% The next version shows this change, but became less clear to read as a result:
% 
% \begin{MyCoqExampleStar}
% Definition checkActiveIntervals st pos
%   : { st' : ScanStateDesc & nextInterval st' = nextInterval st } :=
%   let fix go st st0 H ints pos :=
%     match ints with
%     | nil => existT _ st0 eq_refl
%     | x :: xs =>
%         let i := getInterval (projT1 x) in
%         let p :=
%             if intervalEnd i < pos
%             then let st1 := moveActiveToHandled (projT1 x) (projT2 x) in
%                  let H'  := moveActiveToHandled_spec1 st st1 _ _ eq_refl in
%                  existT _ st1 H'
%             else if negb (intervalCoversPos i pos)
%                  then let st1 := moveActiveToInactive (projT1 x) (projT2 x) in
%                       let H'  := moveActiveToInactive_spec1 st st1 _ _
%                                                             eq_refl in
%                       existT _ st1 H'
%                  else existT _ st0 eq_refl in
%         go st (projT1 p) (projT2 p) xs pos
%     end in
%   go st st eq_refl (activeIntervals st) pos.
% \end{MyCoqExampleStar}

% To give a picture of how this and subsequent design choices evolved, let us
% consider the implementation of a quicksort, written in a fashion similar to
% our register allocator's design at each phase.  First, the purely
% computational approach:

\begin{MyCoqEval}
Require Import Coq.Arith.Compare_dec.
Require Import Coq.Lists.List.
Require Import Coq.Program.Wf.
Require Import Coq.Sorting.Permutation.
Require Import Coq.Sorting.Sorted.

Definition gtb x y :=match nat_compare x y with Gt => true | _ => false end.

Definition definition_goes_here {a : Type} : a. Admitted.
\end{MyCoqEval}

\subsubsection*{Lesson learned}

Proving purely computational algorithms of this complexity meant using proof
scripts to explore every possibility at each step of the algorithm: in effect,
re-implementing the same algorithm in reverse, only now using tactics.

\subsection{Second design: Proof-carrying data}
\label{sec:depinduct}

In the second design of our algorithm, an attempt was made to reduce the
complexity of proof scripts by adding evidence to the primary data structures,
so that such evidence would be available wherever needed.  Take for example an
excerpt from an earlier version of the \texttt{ScanState} data structure,
which records the overall state of several intermediary lists:
\gts{maybe call the record type ``ScanStateV2''?}

\begin{MyCoqExampleStar}
Record OldScanState := {
    unhandled : list (nat * nat);
    active    : list nat;
    inactive  : list nat;
    handled   : list nat;

    unhandled_sorted : StronglySorted (lebf snd) unhandled;
    lists_are_unique : uniq (map fst unhandled ++ active ++
                             inactive ++ handled)
}.
\end{MyCoqExampleStar}

In this structure we have four working lists, and two properties about
the lists. Property \texttt{unhandled\_sorted} requires that the
\texttt{unhandled} list is sorted by start position
(2\textsuperscript{nd} element of each pair in list), and property
\texttt{lists\_are\_unique} states that no interval index recurs
across the four lists.

\subsubsection*{Lessons learned}

While the idea of simultaneously maintaining data structures and
invariants of those data structures is good, mixing data and
propositions into one type does not quite work out, for the following
reasons: 

\begin{itemize}
\item It conflates data with predicates, which is generally a poor choice,
  because the number of predicates often grows over time.

\item It forces the record type to dwell in \texttt{Type} rather than
  \texttt{Set}, and so its inhabitants cannot be used by functions or data
  structures restricted to \texttt{Set}.

\item Any time such values must be mutated, the correctness of the mutation
  must be separately proven \emph{at each instance where it occurs}.  That is,
  the properties cannot be proven generally, but must be constantly re-proven
  in each new context.

\item It forces the propositions to be considered whenever a value is mutated
  or created, even though it may be preferable to transform such values
  successively through different functions, and prove correctness of the
  composed mutation afterward.

  For example, if one were to pass around a sorted list structure, which
  combined a list with a proof of sortedness, it would disallow the
  possibility of unsorted intermediate state, which may still be corrected if
  the overall operation can prove that sortedness is manitained.
\end{itemize}

A guiding principle is to separate concerns as much as practical, which led
directly to the third redesign, discussed in the next section.  

\subsection{Third design: Proof-Qualified Data (Regular Datatypes Qualified by
  Inductive Propositions)}
\label{sec:splitdesign}

After several iterations, where proving was found to be difficult and
the resulting code looked quite unlike its non-dependent alternatives,
we developed a methodology that is the primary message of this
experience report.  Note that this methodology is in no way novel or
unknown, but we find it under-documented in the tutorial literature on
Coq and so present it here to benefit others seeking to use Coq for
verified programming tasks:

\begin{enumerate}
\item Design data-carrying types as inductive data types and records in
  \texttt{Set}, much like in a non-dependently typed language.

\item Constrain how such types may be constructed and mutated by encoding them
  as inductive propositions (in \texttt{Prop}).  For example, the
  \texttt{StronglySorted} inductive predicate found in the Coq standard
  library.

\item High-level functions should take and produce data qualified by these
  propositions; lower-level worker functions may work on unqualifed,
  intermediate values, so long as the composition is qualified.

\item Prove theorems about the properties required of the data, by induction
  on values qualified by the predicate(s).

\item (minor point) If the only role of a proposition is to indicate
  that a property is held or not (for instance, that a list has a
  certain length), it is better to do so using a boolean-returning
  function called from an existential data type that pairs the
  qualified value with the function's result: an example would be
  \verb@{ l : list a | size l == 4 }@, or specialized by a data
  constructor as:

\begin{MyCoqExampleStar}
Inductive listn {a} := mklistn l n of (@size a l == n).
\end{MyCoqExampleStar}
  
  This enables use of small-scale reflection (see the \textsf{ssreflect}
  library), which has benefits beyond the scope of this paper.  We mention it
  here only to underscore that we do not recommend all propositions be made
  inductive, but only those more information rich than simple yes/no properties.

\end{enumerate}

Thus, the third iteration of our design divided each of the core data types
into pairs:

\begin{itemize}
\item A non-dependent record, carrying computationally-relevant content only,
  in \texttt{Set}.

\item Dependent, inductive types, carrying propositionally-relevant content
  only, in \texttt{Prop}.
\end{itemize}

The pairing of the two provides the ``proof-carrying data'' that
was original desired in the second design, but in a manner allowing for simple
transformation of the underlying data when necessary, and induction on
propositional evidence as required.  The division of labor clarified not only
the proofs, but made many constructions far simpler to manage.  This is the
final design that was settled upon, with one additional twist, to be continued
in the next section.

\subsection{Examples}
\label{sec:examplesv3}

As before, we define a datatype \emph{ScanStateDesc} that contains
only data (no propositions). An excerpt is:
\begin{verbatim}
Record ScanStateDesc : Type := {
    unhandled : list (IntervalId * nat);   (* starts after pos *)
    active    : list IntervalId;           (* ranges over pos *)
    inactive  : list IntervalId;           (* falls in lifetime hole *)

    all_state_lists := map fst unhandledIds ++
                       active ++ inactive ++ handled
}.
\end{verbatim}

Next, we define an inductive proposition named \texttt{ScanState}(of type
Proposition, indexed by \texttt{ScanStateDesc}) that enumerates exactly the
allowed methods for transitioning from one \emph{ScanStateDesc} record
satisfying \texttt{ScanState} to another \emph{ScanStateDesc} record
satisfying \texttt{ScanState}. Following are some excerpts:

\begin{verbatim}
Inductive ScanState : ScanStateDesc -> Prop :=
  | ScanState_nil :
    ScanState
      {| ... to fields to 0 or nil... |}
...
  | ScanState_moveActiveToHandled sd :
    ScanState sd -> forall `(H : x \in active sd),
    ScanState
      {| active           := rem x (active sd)
       ; handled          := x :: handled sd
         ... other fields unchanged ... |}
\end{verbatim}

Now, if we want to prove a theorem about a \texttt{ScanState}-qualified
\texttt{ScanStateDesc} record, we consider each case in the definition of
\texttt{ScanState}.

% For example, to prove that the unhandled interval list is sorted, the proof
% is:
% 
% \begin{verbatim}
% Theorem unhandled_sorted `(st : ScanState sd) :
%   StronglySorted (lebf snd) (unhandled sd).
% Proof.
%   ScanState_cases (induction st) Case; simpl in *; try apply IHst.
%   - constructor.
%   - rewrite /unh.
%     by apply/StronglySorted_insert_spec/StronglySorted_widen/IHst.
%   - inv IHst.
% Qed.
% \end{verbatim}

% \subsubsection*{Lesson learned}

% One of the main lessons learned is that Coq is a fantastic environment
% in which to reason about code and algorithms!  It required me to fully
% grasp every single nook and cranny of this algorithm, including all of
% its unwritten assumptions---details which would be all too easy to
% neglect in a freer language.

% Another lesson is that using Coq well requires both education and
% experience.

% This effort took far longer than expected, and much, much longer than
% the equivalent Haskell would have been to write.  That said, many of
% the worst design mistakes that I made were solely due to inexperience
% with using Coq in this way, and are not likely to be made again on
% future projects.

% The Proof General environment in Emacs, the prooftree visualization
% tool, and the ssreflect library, were all invaluable in providing a
% better experience at using Coq.  I truly felt I was able to ``dialog
% with my types'', leading to a richer understanding and better
% decisions.

\section{Proof morphisms}
\label{sec:pfmorph}

During the course of development, it became clear that not only was evidence
required of certain functions, but also a proof that this evidence maintains a
special relationship with some initial condition.  The primary case of this
was proving well-foundedness of the main algorithm.

Well-foundedness may be demonstrated in Coq 8.4 by establishing a mapping from
some input argument to a natural number, and then proving that this value only
decreases with each recursive call.  If we further view an algorithm as a
composition of smaller functions, then each member of the composition must
either preserve this value, or cause it to decrease at least once.

In order to prove that order is maintained, and that we are ultimately
``productive'' (i.e., a decrease in the initial value, the requirement of
well-foundedness), we encode this requirement in the type of each composed
function.  That is, the type must show that the function's input is $\le$ the
initial value, and that the output is $\le$ or $<$ the initial value.  If at
least one output is $<$, and each subsequently composed function is only
$\le$, we have demonstrated well-foundedness overall.

Such specialized composition---where a context is preserved across function
calls---is a notion captured exactly by monads from category theory.  What we
are describing above is specifically an \emph{indexed state monad}, where a
certain state (a relation with our initial value) is maintained throughout the
composition, and we may specify for each function what its input and output
requirements are of this relation.

Here is the sort of function type we end up with:

\begin{MyCoqExampleStar}
Definition handleInterval {pre}
  : SState pre SSMorphHasLen SSMorphSt (option PhysReg).
\end{MyCoqExampleStar}

This function signature tells us that with regard to some original state
\texttt{pre}, we require that the state upon entering this function is related
to this original state such that there is work yet to be done (i.e., we have
proven productivity), and that the output state demonstrates productivity.
Use of this function alone gives sufficient evidence to prove termination, as
will any composition that includes this function.

Working with a monad like this permits the management of evidentiary state to
happen ``behind the scenes'', clarifying the code by removing explicit proof
management.  In the version of our implementation before this change, proving
these relationships became quite time-consuming, and obscured the real work
being done.

By this means, the final well-foundedness proof of the main algorithm became a
one-liner, while at the same time each component function of the algorithm was
greatly simplified, reading similarly to an equivalent Haskell implementation.

% \begin{verbatim}
% Proof.
%   intros; clear; by case: ssinfo' => ?; case=> /= _ /ltP.
% Qed.
% \end{verbatim}

\section{Code extraction}
\label{sec:extract}

A primary goal of this endeavor, from the very beginning, was extraction to
Haskell.  The question was whether Coq and formal methods would be up to the
task of building a Haskell library in a better way.

About midway through the development, focus was turned toward producing
reasonable Haskell code through Coq's built-in extraction facilities.  These
allow for:

\begin{itemize}
\item Extraction of functions and types (those not in Prop), and their related
  definitions.

\item Association of some types and constructors with native Haskell types.

\item Renaming some functions to use their direct Haskell counterparts,
  requiring that one implicit trust these to be both total and correct.
\end{itemize}

There is unfortunately no published meta-theory for the extraction process
from Coq to Haskell, and indeed some glaring bugs were encountered during the
course of this project (including one where type variables were inexplicably
swapped in a constructor whenever auto-inlining was used).

Even still, the ability to write code in Coq and have it so easily converted
to naive Haskell was incredibly useful, and an area that we hope sees further
development.  The result Haskell code was quite simple and straightforward,
which should lend fairly well to optimization by the GHC compiler.

% ---- Bibliography ----
\bibliographystyle{splncs03}
\bibliography{../../safe}

\end{document}
